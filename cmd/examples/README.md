# Example Data Files

Sample data files for testing format converters with realistic fake data generated by gofakeit.

## Files

All files contain **100 records** with realistic fake data:

### Human-Readable Formats
- **sample-data.jsonl** (16KB) - JSON Lines (plain text)
- **sample-data.jsonl.gz** (5.2KB) - JSONL + Gzip
- **sample-data.jsonl.zst** (5.3KB) - JSONL + Zstandard üèÜ RECOMMENDED
- **sample-data.jsonl.lz4** (7.9KB) - JSONL + LZ4 (fastest)
- **sample-data.csv** (7.3KB) - CSV (plain text)
- **sample-data.csv.gz** (3.9KB) - CSV + Gzip
- **sample-data.csv.zst** (4.0KB) - CSV + Zstandard üèÜ RECOMMENDED
- **sample-data.csv.lz4** (4.4KB) - CSV + LZ4 (fastest)

### Binary Formats
- **sample-data.parquet** (7.9KB) - Parquet + Snappy üèÜ BEST OVERALL
- **sample-data.fb** (15KB) - FlatBuffer (uncompressed, fastest reads)
- **sample-data.fb.lz4** (7.9KB) - FlatBuffer + LZ4

**Note:** No compressed Parquet samples (.parquet.lz4, .parquet.zst) included.
Parquet already has built-in Snappy compression - additional compression gives
minimal benefit (~10-15% smaller) but slower access.

### Data Generator
- **generate-data.go** - Script to regenerate sample data
  - Usage: `go run generate-data.go -count 100`
  - Uses gofakeit v7 with fixed seed (12345) for reproducibility

## Record Structure

Each record contains:
- `id` (int64) - Sequential ID
- `name` (string) - Full name
- `email` (string) - Email address
- `age` (int) - Age 18-80
- `score` (float64) - Score 0-100
- `active` (bool) - Active status
- `category` (string) - Category (Electronics, Books, Clothing, etc.)
- `timestamp` (int64) - Unix timestamp

## Usage Examples

### Convert JSONL to Parquet
```bash
cd ..
./any2parquet examples/sample-data.jsonl examples/sample-data.parquet
```

### Convert CSV to Parquet with LZ4 compression
```bash
./any2parquet --lz4 examples/sample-data.csv examples/sample-data.parquet.lz4
```

### Convert JSONL to FlatBuffer
```bash
./any2fb examples/sample-data.jsonl examples/sample-data.fb
```

### Convert JSONL to FlatBuffer with LZ4 compression
```bash
./any2fb --lz4 examples/sample-data.jsonl examples/sample-data.fb.lz4
```

### Convert Parquet back to JSONL (for inspection)
```bash
# First create Parquet
./any2parquet examples/sample-data.jsonl examples/sample-data.parquet

# Then convert to JSONL
./any2jsonl examples/sample-data.parquet examples/output.jsonl
```

### Convert with compressed output
```bash
# JSONL with Zstandard compression (RECOMMENDED)
./any2jsonl --zst examples/sample-data.parquet examples/sample-data.jsonl.zst

# JSONL with Gzip compression
./any2jsonl --gz examples/sample-data.csv examples/sample-data.jsonl.gz

# JSONL with LZ4 compression (fastest)
./any2jsonl --lz4 examples/sample-data.parquet examples/sample-data.jsonl.lz4
```

## Benchmarks

For performance comparison with 1M records, see:
https://github.com/parf/homebase-go-lib/blob/main/benchmarks/serialization-benchmark-result.md

## Format Recommendations

üèÜ **Parquet** - Best overall (fastest, smallest, industry standard)
- Use for: Everything - APIs, analytics, data warehouses
- Performance: 0.15s read, 0.46s write, 44MB for 1M records

‚ö° **FlatBuffer+LZ4** - Fast reads when Parquet not available
- Use for: Hot data paths, high-frequency reads
- Performance: 0.21s read, 1.11s write, 66MB for 1M records

üìÑ **JSONL+Zstd** - Human-readable with compression
- Use for: Debugging, data inspection, text processing
- Performance: 1.91s read, 0.84s write, 43MB for 1M records
